{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/lib/audioUtils.ts"],"sourcesContent":["/**\n * Audio utility helpers for encoding / decoding audio data\n * between the browser and WebSocket transport.\n */\n\n/** Convert a Blob to a base64-encoded string. */\nexport function blobToBase64(blob: Blob): Promise<string> {\n    return new Promise((resolve, reject) => {\n        const reader = new FileReader();\n        reader.onloadend = () => {\n            const dataUrl = reader.result as string;\n            // Strip the data URL prefix: \"data:audio/webm;base64,...\"\n            const base64 = dataUrl.split(\",\")[1];\n            resolve(base64);\n        };\n        reader.onerror = reject;\n        reader.readAsDataURL(blob);\n    });\n}\n\n/** Decode a base64 string to an ArrayBuffer. */\nexport function base64ToArrayBuffer(base64: string): ArrayBuffer {\n    const binaryString = atob(base64);\n    const len = binaryString.length;\n    const bytes = new Uint8Array(len);\n    for (let i = 0; i < len; i++) {\n        bytes[i] = binaryString.charCodeAt(i);\n    }\n    return bytes.buffer;\n}\n"],"names":[],"mappings":"AAAA;;;CAGC,GAED,+CAA+C;;;;;;AACxC,SAAS,aAAa,IAAU;IACnC,OAAO,IAAI,QAAQ,CAAC,SAAS;QACzB,MAAM,SAAS,IAAI;QACnB,OAAO,SAAS,GAAG;YACf,MAAM,UAAU,OAAO,MAAM;YAC7B,0DAA0D;YAC1D,MAAM,SAAS,QAAQ,KAAK,CAAC,IAAI,CAAC,EAAE;YACpC,QAAQ;QACZ;QACA,OAAO,OAAO,GAAG;QACjB,OAAO,aAAa,CAAC;IACzB;AACJ;AAGO,SAAS,oBAAoB,MAAc;IAC9C,MAAM,eAAe,KAAK;IAC1B,MAAM,MAAM,aAAa,MAAM;IAC/B,MAAM,QAAQ,IAAI,WAAW;IAC7B,IAAK,IAAI,IAAI,GAAG,IAAI,KAAK,IAAK;QAC1B,KAAK,CAAC,EAAE,GAAG,aAAa,UAAU,CAAC;IACvC;IACA,OAAO,MAAM,MAAM;AACvB","debugId":null}},
    {"offset": {"line": 39, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/hooks/useVoice.ts"],"sourcesContent":["\"use client\";\n\nimport { useState, useRef, useCallback, useEffect } from \"react\";\nimport { blobToBase64 } from \"@/lib/audioUtils\";\n\nexport interface UseVoiceOptions {\n    /** Called each time a chunk of audio is captured (base64-encoded). */\n    onAudioChunk?: (base64: string) => void;\n    /** MediaRecorder timeslice in ms (default 100). */\n    timeslice?: number;\n}\n\nexport interface UseVoiceReturn {\n    isListening: boolean;\n    error: string | null;\n    startListening: () => Promise<void>;\n    stopListening: () => void;\n}\n\n/**\n * Hook for live, continuous microphone streaming.\n * Audio is captured via MediaRecorder at a low timeslice and\n * base64-encoded chunks are dispatched in real-time.\n *\n * Cleanup happens automatically on unmount.\n */\nexport function useVoice(options: UseVoiceOptions = {}): UseVoiceReturn {\n    const { onAudioChunk, timeslice = 100 } = options;\n\n    const [isListening, setIsListening] = useState(false);\n    const [error, setError] = useState<string | null>(null);\n\n    const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n    const streamRef = useRef<MediaStream | null>(null);\n    const onAudioChunkRef = useRef(onAudioChunk);\n\n    // Keep callback ref fresh\n    useEffect(() => {\n        onAudioChunkRef.current = onAudioChunk;\n    }, [onAudioChunk]);\n\n    const startListening = useCallback(async () => {\n        setError(null);\n\n        try {\n            const stream = await navigator.mediaDevices.getUserMedia({\n                audio: {\n                    echoCancellation: true,\n                    noiseSuppression: true,\n                    autoGainControl: true,\n                    sampleRate: 16000,\n                },\n            });\n\n            streamRef.current = stream;\n\n            const mimeType = MediaRecorder.isTypeSupported(\"audio/webm;codecs=opus\")\n                ? \"audio/webm;codecs=opus\"\n                : \"audio/webm\";\n\n            const recorder = new MediaRecorder(stream, { mimeType });\n\n            recorder.ondataavailable = async (event) => {\n                if (event.data.size > 0 && onAudioChunkRef.current) {\n                    const base64 = await blobToBase64(event.data);\n                    onAudioChunkRef.current(base64);\n                }\n            };\n\n            recorder.onerror = () => {\n                setError(\"Recording error occurred.\");\n                setIsListening(false);\n            };\n\n            // Start with low timeslice for live streaming feel\n            recorder.start(timeslice);\n            mediaRecorderRef.current = recorder;\n            setIsListening(true);\n        } catch (err) {\n            if (err instanceof DOMException && err.name === \"NotAllowedError\") {\n                setError(\"Microphone access was denied\");\n            } else if (err instanceof DOMException && err.name === \"NotFoundError\") {\n                setError(\"No microphone detected\");\n            } else {\n                setError(\"Could not access microphone\");\n            }\n            setIsListening(false);\n        }\n    }, [timeslice]);\n\n    const stopListening = useCallback(() => {\n        if (\n            mediaRecorderRef.current &&\n            mediaRecorderRef.current.state !== \"inactive\"\n        ) {\n            mediaRecorderRef.current.stop();\n        }\n        if (streamRef.current) {\n            streamRef.current.getTracks().forEach((track) => track.stop());\n            streamRef.current = null;\n        }\n        mediaRecorderRef.current = null;\n        setIsListening(false);\n    }, []);\n\n    // Cleanup on unmount\n    useEffect(() => {\n        return () => {\n            if (\n                mediaRecorderRef.current &&\n                mediaRecorderRef.current.state !== \"inactive\"\n            ) {\n                mediaRecorderRef.current.stop();\n            }\n            if (streamRef.current) {\n                streamRef.current.getTracks().forEach((t) => t.stop());\n            }\n        };\n    }, []);\n\n    return { isListening, error, startListening, stopListening };\n}\n"],"names":[],"mappings":";;;;AAEA;AACA;AAHA;;;AA0BO,SAAS,SAAS,UAA2B,CAAC,CAAC;IAClD,MAAM,EAAE,YAAY,EAAE,YAAY,GAAG,EAAE,GAAG;IAE1C,MAAM,CAAC,aAAa,eAAe,GAAG,IAAA,iNAAQ,EAAC;IAC/C,MAAM,CAAC,OAAO,SAAS,GAAG,IAAA,iNAAQ,EAAgB;IAElD,MAAM,mBAAmB,IAAA,+MAAM,EAAuB;IACtD,MAAM,YAAY,IAAA,+MAAM,EAAqB;IAC7C,MAAM,kBAAkB,IAAA,+MAAM,EAAC;IAE/B,0BAA0B;IAC1B,IAAA,kNAAS,EAAC;QACN,gBAAgB,OAAO,GAAG;IAC9B,GAAG;QAAC;KAAa;IAEjB,MAAM,iBAAiB,IAAA,oNAAW,EAAC;QAC/B,SAAS;QAET,IAAI;YACA,MAAM,SAAS,MAAM,UAAU,YAAY,CAAC,YAAY,CAAC;gBACrD,OAAO;oBACH,kBAAkB;oBAClB,kBAAkB;oBAClB,iBAAiB;oBACjB,YAAY;gBAChB;YACJ;YAEA,UAAU,OAAO,GAAG;YAEpB,MAAM,WAAW,cAAc,eAAe,CAAC,4BACzC,2BACA;YAEN,MAAM,WAAW,IAAI,cAAc,QAAQ;gBAAE;YAAS;YAEtD,SAAS,eAAe,GAAG,OAAO;gBAC9B,IAAI,MAAM,IAAI,CAAC,IAAI,GAAG,KAAK,gBAAgB,OAAO,EAAE;oBAChD,MAAM,SAAS,MAAM,IAAA,wIAAY,EAAC,MAAM,IAAI;oBAC5C,gBAAgB,OAAO,CAAC;gBAC5B;YACJ;YAEA,SAAS,OAAO,GAAG;gBACf,SAAS;gBACT,eAAe;YACnB;YAEA,mDAAmD;YACnD,SAAS,KAAK,CAAC;YACf,iBAAiB,OAAO,GAAG;YAC3B,eAAe;QACnB,EAAE,OAAO,KAAK;YACV,IAAI,eAAe,gBAAgB,IAAI,IAAI,KAAK,mBAAmB;gBAC/D,SAAS;YACb,OAAO,IAAI,eAAe,gBAAgB,IAAI,IAAI,KAAK,iBAAiB;gBACpE,SAAS;YACb,OAAO;gBACH,SAAS;YACb;YACA,eAAe;QACnB;IACJ,GAAG;QAAC;KAAU;IAEd,MAAM,gBAAgB,IAAA,oNAAW,EAAC;QAC9B,IACI,iBAAiB,OAAO,IACxB,iBAAiB,OAAO,CAAC,KAAK,KAAK,YACrC;YACE,iBAAiB,OAAO,CAAC,IAAI;QACjC;QACA,IAAI,UAAU,OAAO,EAAE;YACnB,UAAU,OAAO,CAAC,SAAS,GAAG,OAAO,CAAC,CAAC,QAAU,MAAM,IAAI;YAC3D,UAAU,OAAO,GAAG;QACxB;QACA,iBAAiB,OAAO,GAAG;QAC3B,eAAe;IACnB,GAAG,EAAE;IAEL,qBAAqB;IACrB,IAAA,kNAAS,EAAC;QACN,OAAO;YACH,IACI,iBAAiB,OAAO,IACxB,iBAAiB,OAAO,CAAC,KAAK,KAAK,YACrC;gBACE,iBAAiB,OAAO,CAAC,IAAI;YACjC;YACA,IAAI,UAAU,OAAO,EAAE;gBACnB,UAAU,OAAO,CAAC,SAAS,GAAG,OAAO,CAAC,CAAC,IAAM,EAAE,IAAI;YACvD;QACJ;IACJ,GAAG,EAAE;IAEL,OAAO;QAAE;QAAa;QAAO;QAAgB;IAAc;AAC/D","debugId":null}},
    {"offset": {"line": 137, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/hooks/useWebSocket.ts"],"sourcesContent":["\"use client\";\n\nimport { useState, useRef, useCallback, useEffect } from \"react\";\n\nexport interface WebSocketMessage {\n    type: \"transcript\" | \"audio\" | \"status\" | \"error\";\n    content?: string;\n    audio?: string; // base64-encoded audio\n    role?: \"user\" | \"ai\";\n    status?: string;\n}\n\nexport interface UseWebSocketOptions {\n    url: string;\n    onMessage?: (message: WebSocketMessage) => void;\n    /** Max reconnection attempts (default 5). */\n    maxRetries?: number;\n    /** Whether to auto-connect on mount (default true). */\n    autoConnect?: boolean;\n}\n\nexport interface UseWebSocketReturn {\n    isConnected: boolean;\n    error: string | null;\n    sendMessage: (data: string | ArrayBuffer | Record<string, unknown>) => void;\n    connect: () => void;\n    disconnect: () => void;\n}\n\n/**\n * Persistent WebSocket hook with exponential-backoff reconnection.\n */\nexport function useWebSocket(options: UseWebSocketOptions): UseWebSocketReturn {\n    const { url, onMessage, maxRetries = 5, autoConnect = true } = options;\n\n    const [isConnected, setIsConnected] = useState(false);\n    const [error, setError] = useState<string | null>(null);\n\n    const wsRef = useRef<WebSocket | null>(null);\n    const retriesRef = useRef(0);\n    const reconnectTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);\n    const intentionalCloseRef = useRef(false);\n    const onMessageRef = useRef(onMessage);\n\n    // Keep callback ref fresh without triggering reconnect\n    useEffect(() => {\n        onMessageRef.current = onMessage;\n    }, [onMessage]);\n\n    const connect = useCallback(() => {\n        // Cleanup previous connection\n        if (wsRef.current) {\n            intentionalCloseRef.current = true;\n            wsRef.current.close();\n        }\n\n        intentionalCloseRef.current = false;\n        setError(null);\n\n        try {\n            const ws = new WebSocket(url);\n\n            ws.onopen = () => {\n                setIsConnected(true);\n                setError(null);\n                retriesRef.current = 0;\n            };\n\n            ws.onmessage = (event) => {\n                try {\n                    const data: WebSocketMessage =\n                        typeof event.data === \"string\"\n                            ? JSON.parse(event.data)\n                            : event.data;\n                    onMessageRef.current?.(data);\n                } catch {\n                    // If not JSON, treat as plain text transcript\n                    onMessageRef.current?.({ type: \"transcript\", content: event.data, role: \"ai\" });\n                }\n            };\n\n            ws.onerror = () => {\n                setError(\"WebSocket connection error.\");\n            };\n\n            ws.onclose = () => {\n                setIsConnected(false);\n                wsRef.current = null;\n\n                // Auto-reconnect unless intentionally closed\n                if (!intentionalCloseRef.current && retriesRef.current < maxRetries) {\n                    const delay = Math.min(1000 * 2 ** retriesRef.current, 30000);\n                    retriesRef.current += 1;\n                    setError(`Disconnected. Reconnecting in ${Math.round(delay / 1000)}s…`);\n                    reconnectTimerRef.current = setTimeout(() => {\n                        connect();\n                    }, delay);\n                } else if (retriesRef.current >= maxRetries) {\n                    setError(\"Connection lost. Please refresh the page.\");\n                }\n            };\n\n            wsRef.current = ws;\n        } catch {\n            setError(\"Failed to create WebSocket connection.\");\n        }\n    }, [url, maxRetries]);\n\n    const disconnect = useCallback(() => {\n        intentionalCloseRef.current = true;\n        if (reconnectTimerRef.current) {\n            clearTimeout(reconnectTimerRef.current);\n        }\n        if (wsRef.current) {\n            wsRef.current.close();\n        }\n        setIsConnected(false);\n    }, []);\n\n    const sendMessage = useCallback(\n        (data: string | ArrayBuffer | Record<string, unknown>) => {\n            if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {\n                const payload = typeof data === \"object\" && !(data instanceof ArrayBuffer)\n                    ? JSON.stringify(data)\n                    : data;\n                wsRef.current.send(payload as string | ArrayBuffer);\n            }\n        },\n        []\n    );\n\n    // Auto-connect on mount\n    useEffect(() => {\n        if (autoConnect) {\n            connect();\n        }\n        return () => {\n            disconnect();\n        };\n        // eslint-disable-next-line react-hooks/exhaustive-deps\n    }, []);\n\n    return { isConnected, error, sendMessage, connect, disconnect };\n}\n"],"names":[],"mappings":";;;;AAEA;AAFA;;AAgCO,SAAS,aAAa,OAA4B;IACrD,MAAM,EAAE,GAAG,EAAE,SAAS,EAAE,aAAa,CAAC,EAAE,cAAc,IAAI,EAAE,GAAG;IAE/D,MAAM,CAAC,aAAa,eAAe,GAAG,IAAA,iNAAQ,EAAC;IAC/C,MAAM,CAAC,OAAO,SAAS,GAAG,IAAA,iNAAQ,EAAgB;IAElD,MAAM,QAAQ,IAAA,+MAAM,EAAmB;IACvC,MAAM,aAAa,IAAA,+MAAM,EAAC;IAC1B,MAAM,oBAAoB,IAAA,+MAAM,EAAuC;IACvE,MAAM,sBAAsB,IAAA,+MAAM,EAAC;IACnC,MAAM,eAAe,IAAA,+MAAM,EAAC;IAE5B,uDAAuD;IACvD,IAAA,kNAAS,EAAC;QACN,aAAa,OAAO,GAAG;IAC3B,GAAG;QAAC;KAAU;IAEd,MAAM,UAAU,IAAA,oNAAW,EAAC;QACxB,8BAA8B;QAC9B,IAAI,MAAM,OAAO,EAAE;YACf,oBAAoB,OAAO,GAAG;YAC9B,MAAM,OAAO,CAAC,KAAK;QACvB;QAEA,oBAAoB,OAAO,GAAG;QAC9B,SAAS;QAET,IAAI;YACA,MAAM,KAAK,IAAI,UAAU;YAEzB,GAAG,MAAM,GAAG;gBACR,eAAe;gBACf,SAAS;gBACT,WAAW,OAAO,GAAG;YACzB;YAEA,GAAG,SAAS,GAAG,CAAC;gBACZ,IAAI;oBACA,MAAM,OACF,OAAO,MAAM,IAAI,KAAK,WAChB,KAAK,KAAK,CAAC,MAAM,IAAI,IACrB,MAAM,IAAI;oBACpB,aAAa,OAAO,GAAG;gBAC3B,EAAE,OAAM;oBACJ,8CAA8C;oBAC9C,aAAa,OAAO,GAAG;wBAAE,MAAM;wBAAc,SAAS,MAAM,IAAI;wBAAE,MAAM;oBAAK;gBACjF;YACJ;YAEA,GAAG,OAAO,GAAG;gBACT,SAAS;YACb;YAEA,GAAG,OAAO,GAAG;gBACT,eAAe;gBACf,MAAM,OAAO,GAAG;gBAEhB,6CAA6C;gBAC7C,IAAI,CAAC,oBAAoB,OAAO,IAAI,WAAW,OAAO,GAAG,YAAY;oBACjE,MAAM,QAAQ,KAAK,GAAG,CAAC,OAAO,KAAK,WAAW,OAAO,EAAE;oBACvD,WAAW,OAAO,IAAI;oBACtB,SAAS,CAAC,8BAA8B,EAAE,KAAK,KAAK,CAAC,QAAQ,MAAM,EAAE,CAAC;oBACtE,kBAAkB,OAAO,GAAG,WAAW;wBACnC;oBACJ,GAAG;gBACP,OAAO,IAAI,WAAW,OAAO,IAAI,YAAY;oBACzC,SAAS;gBACb;YACJ;YAEA,MAAM,OAAO,GAAG;QACpB,EAAE,OAAM;YACJ,SAAS;QACb;IACJ,GAAG;QAAC;QAAK;KAAW;IAEpB,MAAM,aAAa,IAAA,oNAAW,EAAC;QAC3B,oBAAoB,OAAO,GAAG;QAC9B,IAAI,kBAAkB,OAAO,EAAE;YAC3B,aAAa,kBAAkB,OAAO;QAC1C;QACA,IAAI,MAAM,OAAO,EAAE;YACf,MAAM,OAAO,CAAC,KAAK;QACvB;QACA,eAAe;IACnB,GAAG,EAAE;IAEL,MAAM,cAAc,IAAA,oNAAW,EAC3B,CAAC;QACG,IAAI,MAAM,OAAO,IAAI,MAAM,OAAO,CAAC,UAAU,KAAK,UAAU,IAAI,EAAE;YAC9D,MAAM,UAAU,OAAO,SAAS,YAAY,CAAC,CAAC,gBAAgB,WAAW,IACnE,KAAK,SAAS,CAAC,QACf;YACN,MAAM,OAAO,CAAC,IAAI,CAAC;QACvB;IACJ,GACA,EAAE;IAGN,wBAAwB;IACxB,IAAA,kNAAS,EAAC;QACN,IAAI,aAAa;YACb;QACJ;QACA,OAAO;YACH;QACJ;IACA,uDAAuD;IAC3D,GAAG,EAAE;IAEL,OAAO;QAAE;QAAa;QAAO;QAAa;QAAS;IAAW;AAClE","debugId":null}},
    {"offset": {"line": 251, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/hooks/useAudioPlayback.ts"],"sourcesContent":["\"use client\";\n\nimport { useState, useRef, useCallback } from \"react\";\nimport { base64ToArrayBuffer } from \"@/lib/audioUtils\";\n\nexport interface UseAudioPlaybackReturn {\n    isPlaying: boolean;\n    enqueueAudio: (base64Audio: string) => Promise<void>;\n    stopPlayback: () => void;\n}\n\n/**\n * Hook that manages a gapless audio playback queue using the Web Audio API.\n * Incoming base64-encoded audio chunks are decoded and scheduled back-to-back.\n */\nexport function useAudioPlayback(): UseAudioPlaybackReturn {\n    const [isPlaying, setIsPlaying] = useState(false);\n\n    const audioContextRef = useRef<AudioContext | null>(null);\n    const nextStartTimeRef = useRef(0);\n    const activeSourcesRef = useRef<AudioBufferSourceNode[]>([]);\n\n    /** Lazily create or resume the AudioContext. */\n    const getContext = useCallback(() => {\n        if (!audioContextRef.current || audioContextRef.current.state === \"closed\") {\n            audioContextRef.current = new AudioContext({ sampleRate: 24000 });\n        }\n        if (audioContextRef.current.state === \"suspended\") {\n            audioContextRef.current.resume();\n        }\n        return audioContextRef.current;\n    }, []);\n\n    const enqueueAudio = useCallback(\n        async (base64Audio: string) => {\n            const ctx = getContext();\n            const arrayBuffer = base64ToArrayBuffer(base64Audio);\n\n            try {\n                const audioBuffer = await ctx.decodeAudioData(arrayBuffer);\n\n                const source = ctx.createBufferSource();\n                source.buffer = audioBuffer;\n                source.connect(ctx.destination);\n\n                // Schedule gapless: play at nextStartTime or now, whichever is later\n                const now = ctx.currentTime;\n                const startAt = Math.max(nextStartTimeRef.current, now);\n                source.start(startAt);\n\n                nextStartTimeRef.current = startAt + audioBuffer.duration;\n                activeSourcesRef.current.push(source);\n                setIsPlaying(true);\n\n                source.onended = () => {\n                    activeSourcesRef.current = activeSourcesRef.current.filter(\n                        (s) => s !== source\n                    );\n                    if (activeSourcesRef.current.length === 0) {\n                        setIsPlaying(false);\n                    }\n                };\n            } catch {\n                // Non-decodable chunk — skip silently\n                console.warn(\"Could not decode audio chunk, skipping.\");\n            }\n        },\n        [getContext]\n    );\n\n    const stopPlayback = useCallback(() => {\n        activeSourcesRef.current.forEach((source) => {\n            try {\n                source.stop();\n            } catch {\n                /* already stopped */\n            }\n        });\n        activeSourcesRef.current = [];\n        nextStartTimeRef.current = 0;\n        setIsPlaying(false);\n    }, []);\n\n    return { isPlaying, enqueueAudio, stopPlayback };\n}\n"],"names":[],"mappings":";;;;AAEA;AACA;AAHA;;;AAeO,SAAS;IACZ,MAAM,CAAC,WAAW,aAAa,GAAG,IAAA,iNAAQ,EAAC;IAE3C,MAAM,kBAAkB,IAAA,+MAAM,EAAsB;IACpD,MAAM,mBAAmB,IAAA,+MAAM,EAAC;IAChC,MAAM,mBAAmB,IAAA,+MAAM,EAA0B,EAAE;IAE3D,8CAA8C,GAC9C,MAAM,aAAa,IAAA,oNAAW,EAAC;QAC3B,IAAI,CAAC,gBAAgB,OAAO,IAAI,gBAAgB,OAAO,CAAC,KAAK,KAAK,UAAU;YACxE,gBAAgB,OAAO,GAAG,IAAI,aAAa;gBAAE,YAAY;YAAM;QACnE;QACA,IAAI,gBAAgB,OAAO,CAAC,KAAK,KAAK,aAAa;YAC/C,gBAAgB,OAAO,CAAC,MAAM;QAClC;QACA,OAAO,gBAAgB,OAAO;IAClC,GAAG,EAAE;IAEL,MAAM,eAAe,IAAA,oNAAW,EAC5B,OAAO;QACH,MAAM,MAAM;QACZ,MAAM,cAAc,IAAA,+IAAmB,EAAC;QAExC,IAAI;YACA,MAAM,cAAc,MAAM,IAAI,eAAe,CAAC;YAE9C,MAAM,SAAS,IAAI,kBAAkB;YACrC,OAAO,MAAM,GAAG;YAChB,OAAO,OAAO,CAAC,IAAI,WAAW;YAE9B,qEAAqE;YACrE,MAAM,MAAM,IAAI,WAAW;YAC3B,MAAM,UAAU,KAAK,GAAG,CAAC,iBAAiB,OAAO,EAAE;YACnD,OAAO,KAAK,CAAC;YAEb,iBAAiB,OAAO,GAAG,UAAU,YAAY,QAAQ;YACzD,iBAAiB,OAAO,CAAC,IAAI,CAAC;YAC9B,aAAa;YAEb,OAAO,OAAO,GAAG;gBACb,iBAAiB,OAAO,GAAG,iBAAiB,OAAO,CAAC,MAAM,CACtD,CAAC,IAAM,MAAM;gBAEjB,IAAI,iBAAiB,OAAO,CAAC,MAAM,KAAK,GAAG;oBACvC,aAAa;gBACjB;YACJ;QACJ,EAAE,OAAM;YACJ,sCAAsC;YACtC,QAAQ,IAAI,CAAC;QACjB;IACJ,GACA;QAAC;KAAW;IAGhB,MAAM,eAAe,IAAA,oNAAW,EAAC;QAC7B,iBAAiB,OAAO,CAAC,OAAO,CAAC,CAAC;YAC9B,IAAI;gBACA,OAAO,IAAI;YACf,EAAE,OAAM;YACJ,mBAAmB,GACvB;QACJ;QACA,iBAAiB,OAAO,GAAG,EAAE;QAC7B,iBAAiB,OAAO,GAAG;QAC3B,aAAa;IACjB,GAAG,EAAE;IAEL,OAAO;QAAE;QAAW;QAAc;IAAa;AACnD","debugId":null}},
    {"offset": {"line": 325, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/components/voice/PulseAnimation.tsx"],"sourcesContent":["\"use client\";\n\ninterface PulseAnimationProps {\n    isActive: boolean;\n}\n\n/**\n * Concentric rings that radiate outward from the center orb\n * during live listening.\n */\nexport default function PulseAnimation({ isActive }: PulseAnimationProps) {\n    if (!isActive) return null;\n\n    return (\n        <div className=\"absolute inset-0 flex items-center justify-center pointer-events-none\">\n            <div className=\"absolute w-full h-full rounded-full bg-[var(--accent)]/15 animate-pulse-ring\" />\n            <div className=\"absolute w-full h-full rounded-full bg-[var(--accent)]/10 animate-pulse-ring-2\" />\n            <div className=\"absolute w-full h-full rounded-full bg-[var(--accent)]/5 animate-pulse-ring-3\" />\n        </div>\n    );\n}\n"],"names":[],"mappings":";;;;;AAAA;;AAUe,SAAS,eAAe,EAAE,QAAQ,EAAuB;IACpE,IAAI,CAAC,UAAU,OAAO;IAEtB,qBACI,8OAAC;QAAI,WAAU;;0BACX,8OAAC;gBAAI,WAAU;;;;;;0BACf,8OAAC;gBAAI,WAAU;;;;;;0BACf,8OAAC;gBAAI,WAAU;;;;;;;;;;;;AAG3B","debugId":null}},
    {"offset": {"line": 369, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/components/voice/MicButton.tsx"],"sourcesContent":["\"use client\";\n\nimport PulseAnimation from \"./PulseAnimation\";\n\ninterface MicButtonProps {\n    isListening: boolean;\n    disabled?: boolean;\n    error?: string | null;\n    onClick: () => void;\n}\n\n/**\n * Clean orb button — no icons, just a glowing circle.\n * Active state shows a breathing glow and pulse rings.\n */\nexport default function MicButton({ isListening, disabled, error, onClick }: MicButtonProps) {\n    return (\n        <div className=\"relative flex items-center justify-center\">\n            {/* Pulse rings behind the orb */}\n            <div className=\"absolute w-44 h-44\">\n                <PulseAnimation isActive={isListening} />\n            </div>\n\n            {/* Orb */}\n            <button\n                id=\"mic-button\"\n                onClick={onClick}\n                disabled={disabled}\n                className={`\n          relative z-10 w-20 h-20 rounded-full\n          transition-all duration-500 ease-out\n          cursor-pointer\n          focus:outline-none focus-visible:ring-2 focus-visible:ring-[var(--accent)]/40 focus-visible:ring-offset-2 focus-visible:ring-offset-[var(--bg-primary)]\n          ${isListening\n                        ? \"bg-gradient-to-br from-amber-700 to-orange-900 animate-breathe scale-110\"\n                        : \"bg-[var(--bg-elevated)] hover:bg-[var(--bg-card)] hover:scale-105 border border-[var(--border-accent)]\"\n                    }\n          ${disabled ? \"opacity-30 cursor-not-allowed\" : \"\"}\n          ${error ? \"ring-1 ring-red-500/40\" : \"\"}\n        `}\n                aria-label={isListening ? \"Stop\" : \"Start\"}\n            >\n                {/* Inner dot — visual anchor instead of an icon */}\n                <span\n                    className={`block w-3 h-3 mx-auto rounded-full transition-all duration-500 ${isListening\n                        ? \"bg-white/90 scale-100\"\n                        : \"bg-[var(--text-tertiary)] scale-75\"\n                        }`}\n                />\n            </button>\n\n            {/* Error message */}\n            {error && (\n                <div className=\"absolute -bottom-12 left-1/2 -translate-x-1/2 whitespace-nowrap animate-fade-in-up\">\n                    <p className=\"px-3 py-1 rounded-lg bg-red-500/10 border border-red-500/20 text-red-300 text-[11px] font-medium\">\n                        {error}\n                    </p>\n                </div>\n            )}\n        </div>\n    );\n}\n"],"names":[],"mappings":";;;;;AAEA;AAFA;;;AAee,SAAS,UAAU,EAAE,WAAW,EAAE,QAAQ,EAAE,KAAK,EAAE,OAAO,EAAkB;IACvF,qBACI,8OAAC;QAAI,WAAU;;0BAEX,8OAAC;gBAAI,WAAU;0BACX,cAAA,8OAAC,wJAAc;oBAAC,UAAU;;;;;;;;;;;0BAI9B,8OAAC;gBACG,IAAG;gBACH,SAAS;gBACT,UAAU;gBACV,WAAW,CAAC;;;;;UAKlB,EAAE,cACc,6EACA,yGACL;UACX,EAAE,WAAW,kCAAkC,GAAG;UAClD,EAAE,QAAQ,2BAA2B,GAAG;QAC1C,CAAC;gBACO,cAAY,cAAc,SAAS;0BAGnC,cAAA,8OAAC;oBACG,WAAW,CAAC,+DAA+D,EAAE,cACvE,0BACA,sCACA;;;;;;;;;;;YAKb,uBACG,8OAAC;gBAAI,WAAU;0BACX,cAAA,8OAAC;oBAAE,WAAU;8BACR;;;;;;;;;;;;;;;;;AAMzB","debugId":null}},
    {"offset": {"line": 448, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/components/voice/StatusIndicator.tsx"],"sourcesContent":["\"use client\";\n\nexport type BotStatus = \"idle\" | \"listening\" | \"processing\" | \"speaking\" | \"error\";\n\ninterface StatusIndicatorProps {\n    status: BotStatus;\n    errorMessage?: string | null;\n}\n\nconst CONFIG: Record<BotStatus, { label: string; dotClass: string; textClass: string }> = {\n    idle: {\n        label: \"Ready\",\n        dotClass: \"bg-[var(--text-tertiary)]\",\n        textClass: \"text-[var(--text-tertiary)]\",\n    },\n    listening: {\n        label: \"Listening\",\n        dotClass: \"bg-[var(--accent)]\",\n        textClass: \"text-[var(--accent)]\",\n    },\n    processing: {\n        label: \"Thinking\",\n        dotClass: \"bg-orange-400\",\n        textClass: \"text-orange-300\",\n    },\n    speaking: {\n        label: \"Speaking\",\n        dotClass: \"bg-yellow-200\",\n        textClass: \"text-yellow-200\",\n    },\n    error: {\n        label: \"Error\",\n        dotClass: \"bg-red-400\",\n        textClass: \"text-red-300\",\n    },\n};\n\n/**\n * Minimal status label — just text and a small dot,\n * with animated indicators for \"thinking\" and \"speaking\".\n */\nexport default function StatusIndicator({ status, errorMessage }: StatusIndicatorProps) {\n    const cfg = CONFIG[status];\n\n    return (\n        <div className={`flex items-center gap-2 text-sm font-medium tracking-widest uppercase ${cfg.textClass} transition-colors duration-500`}>\n            {/* Thinking: bouncing dots */}\n            {status === \"processing\" && (\n                <span className=\"flex gap-[3px]\">\n                    {[0, 1, 2].map((i) => (\n                        <span key={i} className=\"w-1 h-1 rounded-full bg-orange-400 bounce-dot\" />\n                    ))}\n                </span>\n            )}\n\n            {/* Speaking: wave bars */}\n            {status === \"speaking\" && (\n                <span className=\"flex items-center gap-[2px] h-4\">\n                    {[0, 1, 2, 3, 4].map((i) => (\n                        <span key={i} className=\"w-[2px] rounded-full bg-yellow-200 wave-bar\" style={{ height: 6 }} />\n                    ))}\n                </span>\n            )}\n\n            {/* Default: static dot */}\n            {status !== \"processing\" && status !== \"speaking\" && (\n                <span className={`w-1.5 h-1.5 rounded-full ${cfg.dotClass} ${status === \"listening\" ? \"animate-pulse\" : \"\"}`} />\n            )}\n\n            <span>{status === \"error\" && errorMessage ? errorMessage : cfg.label}</span>\n        </div>\n    );\n}\n"],"names":[],"mappings":";;;;;AAAA;;AASA,MAAM,SAAoF;IACtF,MAAM;QACF,OAAO;QACP,UAAU;QACV,WAAW;IACf;IACA,WAAW;QACP,OAAO;QACP,UAAU;QACV,WAAW;IACf;IACA,YAAY;QACR,OAAO;QACP,UAAU;QACV,WAAW;IACf;IACA,UAAU;QACN,OAAO;QACP,UAAU;QACV,WAAW;IACf;IACA,OAAO;QACH,OAAO;QACP,UAAU;QACV,WAAW;IACf;AACJ;AAMe,SAAS,gBAAgB,EAAE,MAAM,EAAE,YAAY,EAAwB;IAClF,MAAM,MAAM,MAAM,CAAC,OAAO;IAE1B,qBACI,8OAAC;QAAI,WAAW,CAAC,sEAAsE,EAAE,IAAI,SAAS,CAAC,+BAA+B,CAAC;;YAElI,WAAW,8BACR,8OAAC;gBAAK,WAAU;0BACX;oBAAC;oBAAG;oBAAG;iBAAE,CAAC,GAAG,CAAC,CAAC,kBACZ,8OAAC;wBAAa,WAAU;uBAAb;;;;;;;;;;YAMtB,WAAW,4BACR,8OAAC;gBAAK,WAAU;0BACX;oBAAC;oBAAG;oBAAG;oBAAG;oBAAG;iBAAE,CAAC,GAAG,CAAC,CAAC,kBAClB,8OAAC;wBAAa,WAAU;wBAA8C,OAAO;4BAAE,QAAQ;wBAAE;uBAA9E;;;;;;;;;;YAMtB,WAAW,gBAAgB,WAAW,4BACnC,8OAAC;gBAAK,WAAW,CAAC,yBAAyB,EAAE,IAAI,QAAQ,CAAC,CAAC,EAAE,WAAW,cAAc,kBAAkB,IAAI;;;;;;0BAGhH,8OAAC;0BAAM,WAAW,WAAW,eAAe,eAAe,IAAI,KAAK;;;;;;;;;;;;AAGhF","debugId":null}},
    {"offset": {"line": 553, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/components/voice/LiveTranscript.tsx"],"sourcesContent":["\"use client\";\n\nimport { useEffect, useRef } from \"react\";\n\nexport interface TranscriptEntry {\n    id: string;\n    role: \"user\" | \"ai\";\n    text: string;\n    timestamp: Date;\n}\n\ninterface LiveTranscriptProps {\n    entries: TranscriptEntry[];\n}\n\n/**\n * Minimal scrolling transcript.\n * Auto-scrolls to bottom. User messages float right, AI float left.\n */\nexport default function LiveTranscript({ entries }: LiveTranscriptProps) {\n    const scrollRef = useRef<HTMLDivElement>(null);\n\n    useEffect(() => {\n        if (scrollRef.current) {\n            scrollRef.current.scrollTop = scrollRef.current.scrollHeight;\n        }\n    }, [entries]);\n\n    if (entries.length === 0) {\n        return (\n            <div className=\"flex-1 flex items-center justify-center\">\n                <div className=\"text-center space-y-3 select-none\">\n                    <p className=\"text-[var(--text-secondary)] text-base font-medium\">\n                        Tap the orb to begin a conversation\n                    </p>\n                    <p className=\"text-[var(--text-tertiary)] text-xs\">\n                        Your live transcript appears here\n                    </p>\n                </div>\n            </div>\n        );\n    }\n\n    return (\n        <div\n            ref={scrollRef}\n            className=\"flex-1 overflow-y-auto px-5 py-5 space-y-3 scroll-smooth\"\n        >\n            {entries.map((entry) => (\n                <div\n                    key={entry.id}\n                    className={`flex animate-fade-in-up ${entry.role === \"user\" ? \"justify-end\" : \"justify-start\"\n                        }`}\n                >\n                    <div\n                        className={`\n              max-w-[75%] px-5 py-3.5 rounded-2xl text-[15px] leading-relaxed\n              ${entry.role === \"user\"\n                                ? \"bg-[var(--user-bubble)] text-[var(--text-primary)] rounded-br-sm\"\n                                : \"bg-[var(--ai-bubble)] text-[var(--text-secondary)] rounded-bl-sm border border-[var(--border-subtle)]\"\n                            }\n            `}\n                    >\n                        <p>{entry.text}</p>\n                        <span className=\"block mt-1.5 text-xs opacity-40 font-medium\">\n                            {entry.timestamp.toLocaleTimeString([], {\n                                hour: \"2-digit\",\n                                minute: \"2-digit\",\n                            })}\n                        </span>\n                    </div>\n                </div>\n            ))}\n        </div>\n    );\n}\n"],"names":[],"mappings":";;;;;AAEA;AAFA;;;AAmBe,SAAS,eAAe,EAAE,OAAO,EAAuB;IACnE,MAAM,YAAY,IAAA,+MAAM,EAAiB;IAEzC,IAAA,kNAAS,EAAC;QACN,IAAI,UAAU,OAAO,EAAE;YACnB,UAAU,OAAO,CAAC,SAAS,GAAG,UAAU,OAAO,CAAC,YAAY;QAChE;IACJ,GAAG;QAAC;KAAQ;IAEZ,IAAI,QAAQ,MAAM,KAAK,GAAG;QACtB,qBACI,8OAAC;YAAI,WAAU;sBACX,cAAA,8OAAC;gBAAI,WAAU;;kCACX,8OAAC;wBAAE,WAAU;kCAAqD;;;;;;kCAGlE,8OAAC;wBAAE,WAAU;kCAAsC;;;;;;;;;;;;;;;;;IAMnE;IAEA,qBACI,8OAAC;QACG,KAAK;QACL,WAAU;kBAET,QAAQ,GAAG,CAAC,CAAC,sBACV,8OAAC;gBAEG,WAAW,CAAC,wBAAwB,EAAE,MAAM,IAAI,KAAK,SAAS,gBAAgB,iBACxE;0BAEN,cAAA,8OAAC;oBACG,WAAW,CAAC;;cAEtB,EAAE,MAAM,IAAI,KAAK,SACG,qEACA,wGACL;YACjB,CAAC;;sCAEW,8OAAC;sCAAG,MAAM,IAAI;;;;;;sCACd,8OAAC;4BAAK,WAAU;sCACX,MAAM,SAAS,CAAC,kBAAkB,CAAC,EAAE,EAAE;gCACpC,MAAM;gCACN,QAAQ;4BACZ;;;;;;;;;;;;eAlBH,MAAM,EAAE;;;;;;;;;;AAyBjC","debugId":null}},
    {"offset": {"line": 655, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/components/voice/VoiceInterface.tsx"],"sourcesContent":["\"use client\";\n\nimport { useState, useCallback, useEffect, useRef } from \"react\";\nimport { useVoice } from \"@/hooks/useVoice\";\nimport { useWebSocket, WebSocketMessage } from \"@/hooks/useWebSocket\";\nimport { useAudioPlayback } from \"@/hooks/useAudioPlayback\";\nimport MicButton from \"./MicButton\";\nimport StatusIndicator, { BotStatus } from \"./StatusIndicator\";\nimport LiveTranscript, { TranscriptEntry } from \"./LiveTranscript\";\n\nconst WS_URL = process.env.NEXT_PUBLIC_WS_URL || \"ws://localhost:8080/ws\";\n\n/**\n * Main Voice Interface — live, continuous conversation.\n *\n * When the user taps the orb, the mic turns on and stays on,\n * streaming audio in real-time. The backend is expected to send\n * transcripts and audio chunks as the conversation progresses.\n * The mic stays active until the user taps again.\n */\nexport default function VoiceInterface() {\n    const [status, setStatus] = useState<BotStatus>(\"idle\");\n    const [transcript, setTranscript] = useState<TranscriptEntry[]>([]);\n    const statusRef = useRef(status);\n\n    useEffect(() => {\n        statusRef.current = status;\n    }, [status]);\n\n    // ── Audio Playback ──\n    const { isPlaying, enqueueAudio, stopPlayback } = useAudioPlayback();\n\n    // ── WebSocket ──\n    const handleWsMessage = useCallback(\n        (message: WebSocketMessage) => {\n            switch (message.type) {\n                case \"transcript\":\n                    if (message.content) {\n                        setTranscript((prev) => [\n                            ...prev,\n                            {\n                                id: `${Date.now()}-${Math.random().toString(36).slice(2, 7)}`,\n                                role: message.role || \"ai\",\n                                text: message.content!,\n                                timestamp: new Date(),\n                            },\n                        ]);\n                        // If AI sent a transcript while listening, keep listening status\n                        if (message.role === \"ai\" && statusRef.current !== \"listening\") {\n                            setStatus(\"idle\");\n                        }\n                    }\n                    break;\n\n                case \"audio\":\n                    if (message.audio) {\n                        setStatus(\"speaking\");\n                        enqueueAudio(message.audio);\n                    }\n                    break;\n\n                case \"status\":\n                    if (message.status === \"processing\") {\n                        setStatus(\"processing\");\n                    } else if (message.status === \"done\") {\n                        // If mic is still on, go back to listening\n                        setStatus(\"idle\");\n                    }\n                    break;\n\n                case \"error\":\n                    setStatus(\"error\");\n                    break;\n            }\n        },\n        [enqueueAudio]\n    );\n\n    const {\n        isConnected,\n        error: wsError,\n        sendMessage,\n    } = useWebSocket({\n        url: WS_URL,\n        onMessage: handleWsMessage,\n        autoConnect: true,\n    });\n\n    // ── Voice (live streaming) ──\n    const handleAudioChunk = useCallback(\n        (base64: string) => {\n            sendMessage({ type: \"audio\", audio: base64 });\n        },\n        [sendMessage]\n    );\n\n    const {\n        isListening,\n        error: micError,\n        startListening,\n        stopListening,\n    } = useVoice({ onAudioChunk: handleAudioChunk });\n\n    // ── Toggle live session ──\n    const toggleSession = useCallback(async () => {\n        if (isListening) {\n            // End live session\n            stopListening();\n            stopPlayback();\n            sendMessage({ type: \"audio_end\" });\n            setStatus(\"idle\");\n        } else {\n            // Start live session — mic stays on\n            stopPlayback();\n            await startListening();\n            setStatus(\"listening\");\n            sendMessage({ type: \"audio_start\" });\n        }\n    }, [isListening, startListening, stopListening, stopPlayback, sendMessage]);\n\n    // Derived: when AI finishes speaking, revert to listening if mic is on\n    const effectiveStatus: BotStatus =\n        status === \"speaking\" && !isPlaying\n            ? isListening\n                ? \"listening\"\n                : \"idle\"\n            : status;\n\n    return (\n        <div className=\"relative z-10 flex flex-col h-screen max-w-xl mx-auto\">\n            {/* ── Header ── */}\n            <header className=\"flex items-center justify-between px-6 py-5\">\n                <h1 className=\"text-base font-semibold tracking-tight text-[var(--text-secondary)]\">\n                    voice\n                </h1>\n\n                {/* Glowing connection dot */}\n                <div className=\"flex items-center gap-2\">\n                    <span\n                        className={`\n              w-2 h-2 rounded-full transition-colors duration-500\n              ${isConnected\n                                ? \"bg-[var(--success)] text-[var(--success)] animate-glow-pulse\"\n                                : \"bg-[var(--error)] text-[var(--error)] animate-glow-pulse\"\n                            }\n            `}\n                    />\n                </div>\n            </header>\n\n            {/* ── Transcript ── */}\n            <LiveTranscript entries={transcript} />\n\n            {/* ── Controls ── */}\n            <div className=\"flex flex-col items-center gap-6 px-6 pt-6 pb-10\">\n                <StatusIndicator\n                    status={effectiveStatus}\n                    errorMessage={wsError || micError}\n                />\n\n                <MicButton\n                    isListening={isListening}\n                    error={micError}\n                    onClick={toggleSession}\n                />\n\n                <p className=\"text-xs text-[var(--text-tertiary)] tracking-wide font-medium\">\n                    {isListening ? \"live · tap to end\" : \"tap to start\"}\n                </p>\n            </div>\n        </div>\n    );\n}\n"],"names":[],"mappings":";;;;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AARA;;;;;;;;;AAUA,MAAM,SAAS,QAAQ,GAAG,CAAC,kBAAkB,IAAI;AAUlC,SAAS;IACpB,MAAM,CAAC,QAAQ,UAAU,GAAG,IAAA,iNAAQ,EAAY;IAChD,MAAM,CAAC,YAAY,cAAc,GAAG,IAAA,iNAAQ,EAAoB,EAAE;IAClE,MAAM,YAAY,IAAA,+MAAM,EAAC;IAEzB,IAAA,kNAAS,EAAC;QACN,UAAU,OAAO,GAAG;IACxB,GAAG;QAAC;KAAO;IAEX,uBAAuB;IACvB,MAAM,EAAE,SAAS,EAAE,YAAY,EAAE,YAAY,EAAE,GAAG,IAAA,oJAAgB;IAElE,kBAAkB;IAClB,MAAM,kBAAkB,IAAA,oNAAW,EAC/B,CAAC;QACG,OAAQ,QAAQ,IAAI;YAChB,KAAK;gBACD,IAAI,QAAQ,OAAO,EAAE;oBACjB,cAAc,CAAC,OAAS;+BACjB;4BACH;gCACI,IAAI,GAAG,KAAK,GAAG,GAAG,CAAC,EAAE,KAAK,MAAM,GAAG,QAAQ,CAAC,IAAI,KAAK,CAAC,GAAG,IAAI;gCAC7D,MAAM,QAAQ,IAAI,IAAI;gCACtB,MAAM,QAAQ,OAAO;gCACrB,WAAW,IAAI;4BACnB;yBACH;oBACD,iEAAiE;oBACjE,IAAI,QAAQ,IAAI,KAAK,QAAQ,UAAU,OAAO,KAAK,aAAa;wBAC5D,UAAU;oBACd;gBACJ;gBACA;YAEJ,KAAK;gBACD,IAAI,QAAQ,KAAK,EAAE;oBACf,UAAU;oBACV,aAAa,QAAQ,KAAK;gBAC9B;gBACA;YAEJ,KAAK;gBACD,IAAI,QAAQ,MAAM,KAAK,cAAc;oBACjC,UAAU;gBACd,OAAO,IAAI,QAAQ,MAAM,KAAK,QAAQ;oBAClC,2CAA2C;oBAC3C,UAAU;gBACd;gBACA;YAEJ,KAAK;gBACD,UAAU;gBACV;QACR;IACJ,GACA;QAAC;KAAa;IAGlB,MAAM,EACF,WAAW,EACX,OAAO,OAAO,EACd,WAAW,EACd,GAAG,IAAA,4IAAY,EAAC;QACb,KAAK;QACL,WAAW;QACX,aAAa;IACjB;IAEA,+BAA+B;IAC/B,MAAM,mBAAmB,IAAA,oNAAW,EAChC,CAAC;QACG,YAAY;YAAE,MAAM;YAAS,OAAO;QAAO;IAC/C,GACA;QAAC;KAAY;IAGjB,MAAM,EACF,WAAW,EACX,OAAO,QAAQ,EACf,cAAc,EACd,aAAa,EAChB,GAAG,IAAA,oIAAQ,EAAC;QAAE,cAAc;IAAiB;IAE9C,4BAA4B;IAC5B,MAAM,gBAAgB,IAAA,oNAAW,EAAC;QAC9B,IAAI,aAAa;YACb,mBAAmB;YACnB;YACA;YACA,YAAY;gBAAE,MAAM;YAAY;YAChC,UAAU;QACd,OAAO;YACH,oCAAoC;YACpC;YACA,MAAM;YACN,UAAU;YACV,YAAY;gBAAE,MAAM;YAAc;QACtC;IACJ,GAAG;QAAC;QAAa;QAAgB;QAAe;QAAc;KAAY;IAE1E,uEAAuE;IACvE,MAAM,kBACF,WAAW,cAAc,CAAC,YACpB,cACI,cACA,SACJ;IAEV,qBACI,8OAAC;QAAI,WAAU;;0BAEX,8OAAC;gBAAO,WAAU;;kCACd,8OAAC;wBAAG,WAAU;kCAAsE;;;;;;kCAKpF,8OAAC;wBAAI,WAAU;kCACX,cAAA,8OAAC;4BACG,WAAW,CAAC;;cAEtB,EAAE,cACkB,iEACA,2DACL;YACjB,CAAC;;;;;;;;;;;;;;;;;0BAMD,8OAAC,wJAAc;gBAAC,SAAS;;;;;;0BAGzB,8OAAC;gBAAI,WAAU;;kCACX,8OAAC,yJAAe;wBACZ,QAAQ;wBACR,cAAc,WAAW;;;;;;kCAG7B,8OAAC,mJAAS;wBACN,aAAa;wBACb,OAAO;wBACP,SAAS;;;;;;kCAGb,8OAAC;wBAAE,WAAU;kCACR,cAAc,sBAAsB;;;;;;;;;;;;;;;;;;AAKzD","debugId":null}},
    {"offset": {"line": 863, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/hooks/useSpeechToText.ts"],"sourcesContent":["\"use client\";\n\nimport { useState, useRef, useCallback, useEffect } from \"react\";\n\nexport type SpeechError =\n    | \"not-supported\"\n    | \"not-allowed\"\n    | \"no-speech\"\n    | \"network\"\n    | \"unknown\"\n    | null;\n\nexport interface UseSpeechToTextOptions {\n    /** BCP-47 language tag, e.g., \"en-US\", \"hi-IN\". Defaults to browser locale. */\n    lang?: string;\n}\n\nexport interface UseSpeechToTextReturn {\n    /** Whether the microphone is actively capturing speech. */\n    isListening: boolean;\n    /** Confirmed, final transcript text. */\n    transcript: string;\n    /** Real-time, unconfirmed text (still being processed). */\n    interimTranscript: string;\n    /** Structured error state. */\n    error: SpeechError;\n    /** Begin listening. Should be called from a direct user action. */\n    startListening: () => void;\n    /** Stop listening and freeze the current transcript. */\n    stopListening: () => void;\n    /** Clear all transcript text. */\n    resetTranscript: () => void;\n}\n\n/**\n * useSpeechToText\n *\n * A robust Web Speech API hook for Next.js (Client Component only).\n *\n * Key behaviours:\n *  - Handles both `window.SpeechRecognition` and `window.webkitSpeechRecognition`\n *    (required for Safari + Chrome compatibility).\n *  - `continuous: true` + `interimResults: true` for a live typing effect.\n *  - Auto-restart on `onend` when `isListening` is still true, preventing\n *    the API from silently timing out during natural speech pauses.\n *  - Clean `abort()` on component unmount to release the microphone.\n */\nexport function useSpeechToText(\n    options: UseSpeechToTextOptions = {}\n): UseSpeechToTextReturn {\n    const { lang = \"en-US\" } = options;\n\n    const [isListening, setIsListening] = useState(false);\n    const [transcript, setTranscript] = useState(\"\");\n    const [interimTranscript, setInterimTranscript] = useState(\"\");\n    const [error, setError] = useState<SpeechError>(null);\n\n    // Stable refs so callbacks never go stale in event handlers\n    const recognitionRef = useRef<SpeechRecognition | null>(null);\n    const isListeningRef = useRef(false);\n    const transcriptRef = useRef(\"\");\n\n    // Keep refs in sync with state\n    useEffect(() => {\n        isListeningRef.current = isListening;\n    }, [isListening]);\n\n    /** Resolve the correct constructor — standard or webkit-prefixed. */\n    const getRecognitionConstructor = useCallback(():\n        | SpeechRecognitionStatic\n        | undefined => {\n        if (typeof window === \"undefined\") return undefined;\n        return window.SpeechRecognition ?? window.webkitSpeechRecognition;\n    }, []);\n\n    /** Build a fresh SpeechRecognition instance with all handlers wired up. */\n    const buildRecognition = useCallback((): SpeechRecognition | null => {\n        const SpeechRecognitionImpl = getRecognitionConstructor();\n        if (!SpeechRecognitionImpl) return null;\n\n        const recognition = new SpeechRecognitionImpl();\n        recognition.continuous = true;       // Don't stop after a single phrase\n        recognition.interimResults = true;   // Surface partial results in real-time\n        recognition.lang = lang;\n\n        recognition.onstart = () => {\n            setError(null);\n        };\n\n        recognition.onresult = (event: SpeechRecognitionEvent) => {\n            let finalChunk = \"\";\n            let interimChunk = \"\";\n\n            for (let i = event.resultIndex; i < event.results.length; i++) {\n                const result = event.results[i];\n                const text = result[0].transcript;\n\n                if (result.isFinal) {\n                    finalChunk += text;\n                } else {\n                    interimChunk += text;\n                }\n            }\n\n            if (finalChunk) {\n                // Append confirmed text to the accumulated transcript\n                transcriptRef.current = (transcriptRef.current + \" \" + finalChunk).trim();\n                setTranscript(transcriptRef.current);\n                setInterimTranscript(\"\"); // Clear interim once finalised\n            } else {\n                setInterimTranscript(interimChunk);\n            }\n        };\n\n        recognition.onerror = (event: SpeechRecognitionErrorEvent) => {\n            switch (event.error) {\n                case \"not-allowed\":\n                case \"service-not-allowed\":\n                    setError(\"not-allowed\");\n                    setIsListening(false);\n                    isListeningRef.current = false;\n                    break;\n                case \"no-speech\":\n                    // Non-fatal — auto-restart handles this silently\n                    setError(\"no-speech\");\n                    break;\n                case \"network\":\n                    setError(\"network\");\n                    break;\n                default:\n                    setError(\"unknown\");\n            }\n        };\n\n        /**\n         * Auto-restart on end:\n         * The Web Speech API stops spontaneously after speech pauses or timeouts.\n         * We re-call `start()` immediately if we still *want* to be listening,\n         * creating a seamless, persistent \"live\" experience.\n         */\n        recognition.onend = () => {\n            setInterimTranscript(\"\"); // Clear any leftover interim on each restart\n            if (isListeningRef.current) {\n                try {\n                    recognition.start();\n                } catch {\n                    // If the instance is already started (race condition), ignore.\n                }\n            }\n        };\n\n        return recognition;\n    }, [getRecognitionConstructor, lang]);\n\n    const startListening = useCallback(() => {\n        const SpeechRecognitionImpl = getRecognitionConstructor();\n\n        if (!SpeechRecognitionImpl) {\n            setError(\"not-supported\");\n            return;\n        }\n\n        // Abort any previous instance cleanly\n        if (recognitionRef.current) {\n            recognitionRef.current.abort();\n        }\n\n        const recognition = buildRecognition();\n        if (!recognition) {\n            setError(\"not-supported\");\n            return;\n        }\n\n        recognitionRef.current = recognition;\n        isListeningRef.current = true;\n        setIsListening(true);\n        setError(null);\n\n        try {\n            recognition.start();\n        } catch {\n            setError(\"unknown\");\n            setIsListening(false);\n            isListeningRef.current = false;\n        }\n    }, [getRecognitionConstructor, buildRecognition]);\n\n    const stopListening = useCallback(() => {\n        isListeningRef.current = false;\n        setIsListening(false);\n        setInterimTranscript(\"\");\n\n        if (recognitionRef.current) {\n            recognitionRef.current.stop(); // Graceful stop (flushes final result)\n            recognitionRef.current = null;\n        }\n    }, []);\n\n    const resetTranscript = useCallback(() => {\n        transcriptRef.current = \"\";\n        setTranscript(\"\");\n        setInterimTranscript(\"\");\n    }, []);\n\n    // Cleanup on unmount — release the mic immediately\n    useEffect(() => {\n        return () => {\n            if (recognitionRef.current) {\n                isListeningRef.current = false;\n                recognitionRef.current.abort();\n                recognitionRef.current = null;\n            }\n        };\n    }, []);\n\n    return {\n        isListening,\n        transcript,\n        interimTranscript,\n        error,\n        startListening,\n        stopListening,\n        resetTranscript,\n    };\n}\n"],"names":[],"mappings":";;;;AAEA;AAFA;;AA+CO,SAAS,gBACZ,UAAkC,CAAC,CAAC;IAEpC,MAAM,EAAE,OAAO,OAAO,EAAE,GAAG;IAE3B,MAAM,CAAC,aAAa,eAAe,GAAG,IAAA,iNAAQ,EAAC;IAC/C,MAAM,CAAC,YAAY,cAAc,GAAG,IAAA,iNAAQ,EAAC;IAC7C,MAAM,CAAC,mBAAmB,qBAAqB,GAAG,IAAA,iNAAQ,EAAC;IAC3D,MAAM,CAAC,OAAO,SAAS,GAAG,IAAA,iNAAQ,EAAc;IAEhD,4DAA4D;IAC5D,MAAM,iBAAiB,IAAA,+MAAM,EAA2B;IACxD,MAAM,iBAAiB,IAAA,+MAAM,EAAC;IAC9B,MAAM,gBAAgB,IAAA,+MAAM,EAAC;IAE7B,+BAA+B;IAC/B,IAAA,kNAAS,EAAC;QACN,eAAe,OAAO,GAAG;IAC7B,GAAG;QAAC;KAAY;IAEhB,mEAAmE,GACnE,MAAM,4BAA4B,IAAA,oNAAW,EAAC;QAG1C,wCAAmC,OAAO;;;IAE9C,GAAG,EAAE;IAEL,yEAAyE,GACzE,MAAM,mBAAmB,IAAA,oNAAW,EAAC;QACjC,MAAM,wBAAwB;QAC9B,IAAI,CAAC,uBAAuB,OAAO;QAEnC,MAAM,cAAc,IAAI;QACxB,YAAY,UAAU,GAAG,MAAY,mCAAmC;QACxE,YAAY,cAAc,GAAG,MAAQ,uCAAuC;QAC5E,YAAY,IAAI,GAAG;QAEnB,YAAY,OAAO,GAAG;YAClB,SAAS;QACb;QAEA,YAAY,QAAQ,GAAG,CAAC;YACpB,IAAI,aAAa;YACjB,IAAI,eAAe;YAEnB,IAAK,IAAI,IAAI,MAAM,WAAW,EAAE,IAAI,MAAM,OAAO,CAAC,MAAM,EAAE,IAAK;gBAC3D,MAAM,SAAS,MAAM,OAAO,CAAC,EAAE;gBAC/B,MAAM,OAAO,MAAM,CAAC,EAAE,CAAC,UAAU;gBAEjC,IAAI,OAAO,OAAO,EAAE;oBAChB,cAAc;gBAClB,OAAO;oBACH,gBAAgB;gBACpB;YACJ;YAEA,IAAI,YAAY;gBACZ,sDAAsD;gBACtD,cAAc,OAAO,GAAG,CAAC,cAAc,OAAO,GAAG,MAAM,UAAU,EAAE,IAAI;gBACvE,cAAc,cAAc,OAAO;gBACnC,qBAAqB,KAAK,+BAA+B;YAC7D,OAAO;gBACH,qBAAqB;YACzB;QACJ;QAEA,YAAY,OAAO,GAAG,CAAC;YACnB,OAAQ,MAAM,KAAK;gBACf,KAAK;gBACL,KAAK;oBACD,SAAS;oBACT,eAAe;oBACf,eAAe,OAAO,GAAG;oBACzB;gBACJ,KAAK;oBACD,iDAAiD;oBACjD,SAAS;oBACT;gBACJ,KAAK;oBACD,SAAS;oBACT;gBACJ;oBACI,SAAS;YACjB;QACJ;QAEA;;;;;SAKC,GACD,YAAY,KAAK,GAAG;YAChB,qBAAqB,KAAK,6CAA6C;YACvE,IAAI,eAAe,OAAO,EAAE;gBACxB,IAAI;oBACA,YAAY,KAAK;gBACrB,EAAE,OAAM;gBACJ,+DAA+D;gBACnE;YACJ;QACJ;QAEA,OAAO;IACX,GAAG;QAAC;QAA2B;KAAK;IAEpC,MAAM,iBAAiB,IAAA,oNAAW,EAAC;QAC/B,MAAM,wBAAwB;QAE9B,IAAI,CAAC,uBAAuB;YACxB,SAAS;YACT;QACJ;QAEA,sCAAsC;QACtC,IAAI,eAAe,OAAO,EAAE;YACxB,eAAe,OAAO,CAAC,KAAK;QAChC;QAEA,MAAM,cAAc;QACpB,IAAI,CAAC,aAAa;YACd,SAAS;YACT;QACJ;QAEA,eAAe,OAAO,GAAG;QACzB,eAAe,OAAO,GAAG;QACzB,eAAe;QACf,SAAS;QAET,IAAI;YACA,YAAY,KAAK;QACrB,EAAE,OAAM;YACJ,SAAS;YACT,eAAe;YACf,eAAe,OAAO,GAAG;QAC7B;IACJ,GAAG;QAAC;QAA2B;KAAiB;IAEhD,MAAM,gBAAgB,IAAA,oNAAW,EAAC;QAC9B,eAAe,OAAO,GAAG;QACzB,eAAe;QACf,qBAAqB;QAErB,IAAI,eAAe,OAAO,EAAE;YACxB,eAAe,OAAO,CAAC,IAAI,IAAI,uCAAuC;YACtE,eAAe,OAAO,GAAG;QAC7B;IACJ,GAAG,EAAE;IAEL,MAAM,kBAAkB,IAAA,oNAAW,EAAC;QAChC,cAAc,OAAO,GAAG;QACxB,cAAc;QACd,qBAAqB;IACzB,GAAG,EAAE;IAEL,mDAAmD;IACnD,IAAA,kNAAS,EAAC;QACN,OAAO;YACH,IAAI,eAAe,OAAO,EAAE;gBACxB,eAAe,OAAO,GAAG;gBACzB,eAAe,OAAO,CAAC,KAAK;gBAC5B,eAAe,OAAO,GAAG;YAC7B;QACJ;IACJ,GAAG,EAAE;IAEL,OAAO;QACH;QACA;QACA;QACA;QACA;QACA;QACA;IACJ;AACJ","debugId":null}},
    {"offset": {"line": 1029, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/src/components/voice/VoiceInput.tsx"],"sourcesContent":["\"use client\";\n\nimport { useSpeechToText } from \"@/hooks/useSpeechToText\";\n\nconst ERROR_MESSAGES: Record<string, string> = {\n    \"not-supported\": \"Your browser doesn't support speech recognition. Try Chrome or Edge.\",\n    \"not-allowed\": \"Microphone access was denied. Please allow it in your browser settings.\",\n    \"no-speech\": \"No speech detected. Please try speaking again.\",\n    network: \"Network error. Check your connection and try again.\",\n    unknown: \"An unexpected error occurred. Please try again.\",\n};\n\n/**\n * VoiceInput\n *\n * Standalone visual component built on useSpeechToText.\n * Shows a pulsing record button, live interim text, and confirmed transcript.\n * Designed to integrate cleanly with the existing Chocolate Truffle theme.\n */\nexport default function VoiceInput() {\n    const {\n        isListening,\n        transcript,\n        interimTranscript,\n        error,\n        startListening,\n        stopListening,\n        resetTranscript,\n    } = useSpeechToText({ lang: \"en-US\" });\n\n    const isEmpty = !transcript && !interimTranscript;\n\n    return (\n        <section className=\"relative z-10 w-full max-w-xl mx-auto px-6 pb-10\">\n            {/* ── Divider ── */}\n            <div className=\"flex items-center gap-3 mb-8\">\n                <div className=\"flex-1 h-px bg-[var(--border-accent)]\" />\n                <span className=\"text-[11px] text-[var(--text-tertiary)] uppercase tracking-widest select-none\">\n                    speech capture\n                </span>\n                <div className=\"flex-1 h-px bg-[var(--border-accent)]\" />\n            </div>\n\n            {/* ── Transcript Display ── */}\n            <div\n                className={`\n          relative w-full min-h-[120px] rounded-2xl px-5 py-4 mb-6\n          border transition-all duration-300\n          ${isListening\n                        ? \"border-[var(--accent)]/40 bg-[var(--bg-elevated)]\"\n                        : \"border-[var(--border-subtle)] bg-[var(--bg-card)]\"\n                    }\n        `}\n            >\n                {/* Recording indicator halo */}\n                {isListening && (\n                    <span className=\"absolute top-3 right-3 flex h-2.5 w-2.5\">\n                        <span className=\"absolute inline-flex h-full w-full rounded-full bg-red-400 opacity-75 animate-ping\" />\n                        <span className=\"relative inline-flex rounded-full h-2.5 w-2.5 bg-red-500\" />\n                    </span>\n                )}\n\n                {/* Placeholder when empty */}\n                {isEmpty && !isListening && (\n                    <p className=\"text-base text-[var(--text-tertiary)] font-light select-none\">\n                        Tap the button below and start speaking...\n                    </p>\n                )}\n\n                {isEmpty && isListening && (\n                    <p className=\"text-base text-[var(--text-tertiary)] italic animate-pulse select-none\">\n                        Listening...\n                    </p>\n                )}\n\n                {/* Transcript content */}\n                {!isEmpty && (\n                    <p className=\"text-base leading-relaxed font-light\">\n                        {/* Final transcript — full opacity, confirmed */}\n                        {transcript && (\n                            <span className=\"text-[var(--text-primary)] font-normal\">\n                                {transcript}\n                                {interimTranscript ? \" \" : \"\"}\n                            </span>\n                        )}\n                        {/* Interim transcript — muted, still being processed */}\n                        {interimTranscript && (\n                            <span className=\"text-[var(--text-tertiary)] italic\">\n                                {interimTranscript}\n                            </span>\n                        )}\n                    </p>\n                )}\n            </div>\n\n            {/* ── Error Message ── */}\n            {error && error !== \"no-speech\" && (\n                <div className=\"mb-5 px-4 py-3 rounded-xl bg-[var(--error)]/10 border border-[var(--error)]/20 animate-fade-in-up\">\n                    <p className=\"text-sm text-[var(--error)]\">\n                        {ERROR_MESSAGES[error] ?? ERROR_MESSAGES.unknown}\n                    </p>\n                </div>\n            )}\n\n            {/* ── Controls ── */}\n            <div className=\"flex items-center gap-4\">\n                {/* Record / Stop Button */}\n                <button\n                    id=\"voice-record-button\"\n                    onClick={isListening ? stopListening : startListening}\n                    className={`\n            relative flex items-center justify-center gap-2.5\n            px-5 py-3 rounded-full text-sm font-medium\n            transition-all duration-300 ease-out select-none\n            focus:outline-none focus-visible:ring-2 focus-visible:ring-[var(--accent)]/40\n            ${isListening\n                            ? \"bg-red-500/95 hover:bg-red-600 text-white\"\n                            : \"bg-[var(--accent)] hover:opacity-90 text-white\"\n                        }\n          `}\n                    aria-label={isListening ? \"Stop recording\" : \"Start recording\"}\n                >\n                    {/* Pulsing outer ring when active */}\n                    {isListening && (\n                        <span className=\"absolute inset-0 rounded-full border-2 border-red-400 opacity-60 animate-pulse-ring pointer-events-none\" />\n                    )}\n\n                    {/* Dot indicator */}\n                    <span\n                        className={`\n              flex-shrink-0 w-2.5 h-2.5 rounded-full transition-all duration-300\n              ${isListening ? \"bg-white animate-pulse\" : \"bg-white/80\"}\n            `}\n                    />\n                    {isListening ? \"Stop\" : \"Record\"}\n                </button>\n\n                {/* Clear button — only shown when there's something to clear */}\n                {(transcript || interimTranscript) && (\n                    <button\n                        id=\"voice-clear-button\"\n                        onClick={resetTranscript}\n                        className=\"px-4 py-3 rounded-full text-sm font-medium text-[var(--text-secondary)] hover:text-[var(--text-primary)] border border-[var(--border-accent)] hover:border-[var(--border-subtle)] transition-all duration-200 select-none\"\n                        aria-label=\"Clear transcript\"\n                    >\n                        Clear\n                    </button>\n                )}\n\n                {/* Live word count */}\n                {!isEmpty && (\n                    <span className=\"ml-auto text-xs text-[var(--text-tertiary)] select-none\">\n                        {[transcript, interimTranscript]\n                            .join(\" \")\n                            .trim()\n                            .split(/\\s+/)\n                            .filter(Boolean).length}{\" \"}\n                        words\n                    </span>\n                )}\n            </div>\n        </section>\n    );\n}\n"],"names":[],"mappings":";;;;;AAEA;AAFA;;;AAIA,MAAM,iBAAyC;IAC3C,iBAAiB;IACjB,eAAe;IACf,aAAa;IACb,SAAS;IACT,SAAS;AACb;AASe,SAAS;IACpB,MAAM,EACF,WAAW,EACX,UAAU,EACV,iBAAiB,EACjB,KAAK,EACL,cAAc,EACd,aAAa,EACb,eAAe,EAClB,GAAG,IAAA,kJAAe,EAAC;QAAE,MAAM;IAAQ;IAEpC,MAAM,UAAU,CAAC,cAAc,CAAC;IAEhC,qBACI,8OAAC;QAAQ,WAAU;;0BAEf,8OAAC;gBAAI,WAAU;;kCACX,8OAAC;wBAAI,WAAU;;;;;;kCACf,8OAAC;wBAAK,WAAU;kCAAgF;;;;;;kCAGhG,8OAAC;wBAAI,WAAU;;;;;;;;;;;;0BAInB,8OAAC;gBACG,WAAW,CAAC;;;UAGlB,EAAE,cACc,sDACA,oDACL;QACb,CAAC;;oBAGQ,6BACG,8OAAC;wBAAK,WAAU;;0CACZ,8OAAC;gCAAK,WAAU;;;;;;0CAChB,8OAAC;gCAAK,WAAU;;;;;;;;;;;;oBAKvB,WAAW,CAAC,6BACT,8OAAC;wBAAE,WAAU;kCAA+D;;;;;;oBAK/E,WAAW,6BACR,8OAAC;wBAAE,WAAU;kCAAyE;;;;;;oBAMzF,CAAC,yBACE,8OAAC;wBAAE,WAAU;;4BAER,4BACG,8OAAC;gCAAK,WAAU;;oCACX;oCACA,oBAAoB,MAAM;;;;;;;4BAIlC,mCACG,8OAAC;gCAAK,WAAU;0CACX;;;;;;;;;;;;;;;;;;YAQpB,SAAS,UAAU,6BAChB,8OAAC;gBAAI,WAAU;0BACX,cAAA,8OAAC;oBAAE,WAAU;8BACR,cAAc,CAAC,MAAM,IAAI,eAAe,OAAO;;;;;;;;;;;0BAM5D,8OAAC;gBAAI,WAAU;;kCAEX,8OAAC;wBACG,IAAG;wBACH,SAAS,cAAc,gBAAgB;wBACvC,WAAW,CAAC;;;;;YAKpB,EAAE,cACgB,8CACA,iDACL;UACf,CAAC;wBACS,cAAY,cAAc,mBAAmB;;4BAG5C,6BACG,8OAAC;gCAAK,WAAU;;;;;;0CAIpB,8OAAC;gCACG,WAAW,CAAC;;cAEtB,EAAE,cAAc,2BAA2B,cAAc;YAC3D,CAAC;;;;;;4BAEQ,cAAc,SAAS;;;;;;;oBAI3B,CAAC,cAAc,iBAAiB,mBAC7B,8OAAC;wBACG,IAAG;wBACH,SAAS;wBACT,WAAU;wBACV,cAAW;kCACd;;;;;;oBAMJ,CAAC,yBACE,8OAAC;wBAAK,WAAU;;4BACX;gCAAC;gCAAY;6BAAkB,CAC3B,IAAI,CAAC,KACL,IAAI,GACJ,KAAK,CAAC,OACN,MAAM,CAAC,SAAS,MAAM;4BAAE;4BAAI;;;;;;;;;;;;;;;;;;;AAOzD","debugId":null}},
    {"offset": {"line": 1261, "column": 0}, "map": {"version":3,"sources":["file:///Users/yashnandankar/Hackathon/node_modules/next/src/server/route-modules/app-page/vendored/ssr/react-jsx-dev-runtime.ts"],"sourcesContent":["module.exports = (\n  require('../../module.compiled') as typeof import('../../module.compiled')\n).vendored['react-ssr']!.ReactJsxDevRuntime\n"],"names":["module","exports","require","vendored","ReactJsxDevRuntime"],"mappings":"AAAAA,OAAOC,OAAO,GACZC,QAAQ,4HACRC,QAAQ,CAAC,YAAY,CAAEC,kBAAkB","ignoreList":[0],"debugId":null}}]
}